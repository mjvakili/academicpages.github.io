---
title: 'Many flavors of feature importance'
date: 2018-10-30
permalink: /posts/2018/10/blog-post-1/
tags:
  - machine learning -feature engineering -feature importance
---

Let's imagine you are trying to predict whether you need to sell or buy stocks of a certain commidity in the market.
Given the historical time evolution of the price, you can certainly a large number of features.
This includes:

* Returns: the difference between the closing prices of the stock in adjacent days.

* Volatility: the fluctuations (measured by standard deviation) in the returns within a certain period of time.

* Serial Correlations: we can look at how correlated the prices are within a certain period of time.

* Moving Averages of the prices: we can also compute the fast and the slow moving averages of the closing price values.

The list can go one. However, one question remains: Which features should we be using? 

Mean Decrease in Accuracy/Score
======

Mean decrease in accruacy (MDA) ranks the importance of features by removing the dependence of the 
classifer outcome on each feature, one at a time. First, we fit a classifer to our training data, keeping all the features and then we compute the cross-validation accurcy. Then we permutate (randomize the order of) each feature of the data, one feature at a time, computing the accuracy of the model. The importance ranking of the features is then determined by looking at the relative drop in the cross-validation accuracy as a result of randomizing the features. 

Single Feature Importance
======

This method computes the predictive importance of each feature in isolation. 
This is done by computing the cross-validaiton score when only one feature in isolation is used. 
Let's imagine we are training a tree-based classifer in which tree learns to first split the sample on Volatility and then on Moving Average of the prices in the next node. This demonstrates the predictive power of utilizing both features at the same time. The problem with the Single Feature Importance method is that, by construction, it is going to ignore such heirarchical importance of the features. One advantage of this method over mean decrease in accuracy is that it is not sensitive to the correlation between the features. 

Mean decrease (increase) in impurity (entropy)
=======

You have probably noticed the feature_importance_ attribute of the scikit-learn tree-based classifiers.
At every node of a decision tree, the tree splits the data into two branches either based on a feature that lowers the relative mixing between the two subsamples (when the splitting is done with a Gini impurity index) or based on a feature that maximizes the information gain between the child nodes and the parent nodes.

Then the relative importance of the features can be estimated by looking at how much each feature has contributed to the decrease (increase) in impurity (information) in the tree. A point that one may have to keep in mind is that this procedure is done during the training and it may not provide a good picture of the relative importance of features when we look at an out-of-sample performance of a classifer (or regressor).

One issue with this approach is that the impurity (information) decrease (increase) quantity is calculated conditioned on 
the other features of the data. So this method is going to be susceptible to the presence of correlation between different features of the data.

Orthogonal Features
======

In order to alleviate the problem of the presence of correlated features, one may be tempted 
to take a look at the correlations between various features and drop the redundant features. 
In principle, this can be done in more systematic way. 

Let us denote the number data points by N and the number of features by M; then the covariance matrix of the data is an MxM symmetric matrix whose off-diagonal elements are given by the covariance between the different features. We can transform the linear basis to a new orthogonal basis in which the covariance matrix is diagonal. The basis vectors of this new linear basis are the eigen vectors of the covariance matrix. In this new representation, there is no covariance between the different columns of the data (as the covariance matrix is diagonal). That is, the features are orthogonal to each other and there is no covariance between them. In this new representation of the data, one can proceed with either mean decrease in accuracy or mean decrease (increase) in impurity (entropy/information) without having to worry about the covariance between the features. 
One downside of this approach is that the orthognal features maybe less interpretable that the features that one would directly work with in the initial representation of the data

Conclusion
=========

Each feature importance method discussed here has its pros and cons. It usually boils down to in-sample versus out-of-sample performance, covariance between the features, heirarchical importance, and interpretability. Before, choosing a specific method for analyzing feature importance in a problem, one has to take a good look at the various properties of the data at hand. We also need to be aware of the underlying assumptions of these methods and their possible shortcomings.  

