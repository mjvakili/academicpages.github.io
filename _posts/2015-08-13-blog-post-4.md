---
title: 'Information Maximizing Neural Networks'
date: 2018-09-30
permalink: /posts/2018/09/blog-post-1/
tags:
  - deep learning
  - probability theory
  - neural networks
  - likelihood free inference
  - bayesian inference
---

We would like to learn the probability of some model conditioned on the data p(model|data).
In a typical Bayesian setup one would estimate this with a likelihood function p(data|model) and a prior probability 
p(model). This requires making two assumption about the functional form of the likelihood function. 
In cosmological experiements the likelihood function is assumed to be a multivariate Gaussian whose mean and covariance 
are given by the model and the data covariance matrix respectively. There are a few issues here:

* The covariance matrix is an NxN dimensional matrix where N can be as large as 100,000; thus difficult to estimate.
* The functional form of the likelihood might deviate from Gaussianity leading to biases and/or misestimation of uncertainties.

Approximate Bayesian Computation (ABC) alleviates this problem by removing the likelihood function and 
replacing the Bayesian setup with the following steps:

* sample from the model prior
* simulate the data from the sampled model
* compute the distance between the simulated data and the observed data
* if the distance(simulated data , observed data) < threshold:
  * accept the model
* else  
  * reject the model
  
In a high dimensional space the condition distance(simulated data , observed data) < threshold 
is almost impossible to meet. 

